{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Spectrogram classification\n",
        "\n",
        "**Deadline**: 21/2/24\n",
        "\n",
        "**Submission: Submit a PDF export of the completed notebook as well as the .ipynb file.**\n",
        "\n",
        "\n",
        "**General**:\n",
        "This assignment aims to practice designing and training neural networks. The task the networks solve is “predicting”/”inferring” a signal type from its spectrogram image.\n",
        "You will explore two neural network architectures. A starter code is provided to help with data processing and make it a bit easier.\n",
        "\n",
        "You may modify the starter code as you see fit, including changing the signatures of functions and adding/removing helper functions. However, please ensure you adequately explain what you are doing and why."
      ],
      "metadata": {
        "id": "5YWrTV1pbYMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import scipy.io\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "DMZ5hSyRc-_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1. Data (15%)\n",
        "\n",
        "With any machine learning problem, the first thing that we would want to do\n",
        "is to get an intuitive understanding of what our data looks like. Download the file\n",
        "`Data set` from the course page on Moodle and upload it to Google Drive.\n",
        "Then, mount Google Drive from your Google Colab notebook:"
      ],
      "metadata": {
        "id": "r5lMTodVeaez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Find the path to the file:\n",
        "path = '/content/gdrive/My Drive/assignment2' # TODO - UPDATE ME!"
      ],
      "metadata": {
        "id": "rqd_ExJEejS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (a) -- 3%\n",
        "\n",
        "Load the training and test data, and separate the training data into training and validation.\n",
        "Create the NumPy arrays `train_data`, `valid_data`, `test_data`.\n",
        "\n",
        "  \n",
        "\n",
        "1.    `data`, all of which should be of shape `[N, 128, 128, 1]`. The dimensions of this NumPy array are as follows:\n",
        "\n",
        "- `N` - the number of rows allocated to train, valid, or test\n",
        "- `128` - the height of each spectrogram (i.e., the number of freq. points)\n",
        "- `128` - the width of each spectrogram (i.e., the number of time samples)\n",
        "- `1` - the color channels\n",
        "\n",
        "2.   `labels`, all of which should be of shape `[N,]` The dimensions of this NumPy array are as follows:\n",
        "\n",
        "- `N` - the number of rows allocated to train, valid, or test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The pixel intensities are stored as an integer between 0 and 255.\n",
        "Make sure you normalize your images, namely, divide the intensities by 255 so that you have floating-point values between 0 and 1. Then, subtract 0.5\n",
        "so that the elements of `train_data`, `valid_data` and `test_data` are between -0.5 and 0.5.\n",
        "**Note that this step actually makes a huge difference in training!**\n",
        "\n",
        "This function might take a while to run, and it can take several minutes just to load the files from Google Drive. If you want to avoid running this code multiple times, you can save your NumPy arrays and load it later:\n",
        "https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html"
      ],
      "metadata": {
        "id": "zYWjfJV3DuCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "folder_path = '/content/gdrive/My Drive/assignment2'\n",
        "def sort_data(folder_path):\n",
        "  train_path = f'{folder_path}/train_data/*.jpg'\n",
        "  test_path = f'{folder_path}/test_data/*.jpg'\n",
        "  train_images = {}\n",
        "  test_images = {}\n",
        "  for file in glob.glob(train_path):\n",
        "      filename = file.split(\"/\")[-1] # get the name of the .png file\n",
        "      label = filename.split('_')[0] # get the label\n",
        "      image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
        "      img = cv2.resize(image, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "      img = (img/255) - 0.5\n",
        "      train_images[filename] = img\n",
        "\n",
        "  for file in glob.glob(test_path):\n",
        "      filename = file.split(\"/\")[-1] # get the name of the .png file\n",
        "      label = filename.split('_')[0] # get the label\n",
        "      image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
        "      img = cv2.resize(image, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "      img = (img/255) - 0.5\n",
        "      test_images[filename] = img\n",
        "\n",
        "  sort_train_dict = dict(sorted(train_images.items()))\n",
        "  sort_test_dict = dict(sorted(test_images.items()))\n",
        "  train = np.array(list(sort_train_dict.values()))\n",
        "  test = np.array(list(sort_test_dict.values()))\n",
        "  train_label = np.array(list(sort_train_dict.keys()))\n",
        "  test_labels = np.array(list(sort_test_dict.keys()))\n",
        "  reindex = np.random.permutation(len(train))\n",
        "  train = train[reindex]\n",
        "  train_label = train_label[reindex]\n",
        "  tr_label, ts_label = [], []\n",
        "  for ii in range(len(train_label)):\n",
        "    tr_label.append(train_label[ii].split('_')[0])\n",
        "  for ii in range(len(test_labels)):\n",
        "    ts_label.append(test_labels[ii].split('_')[0])\n",
        "  train_label = np.array(tr_label)\n",
        "  test_labels = np.array(ts_label)\n",
        "  train_data, valid_data = train[int(train.shape[0] * 0.15):], train[:int(train.shape[0] * 0.15)]\n",
        "  train_labels, valid_labels = train_label[int(train_label.shape[0] * 0.15):], train_label[:int(train_label.shape[0] * 0.15)]\n",
        "  return train_data, train_labels, valid_data, valid_labels, test, test_labels"
      ],
      "metadata": {
        "id": "8I0QBSI3Lh62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (b) -- 3%\n",
        "\n",
        "We want to train a model that determines the signal type from a spectrogram. Therefore, our model will take in a spectrogram image.\n",
        "\n",
        "Write a function generate_plots() that takes one of the data sets that you produced in part (a), and generates image plots of the different spectrograms with different classes. Your function generate_plots() plots 12 subplots of spectrogram images containing all classes.\n",
        "\n",
        "Note: While at this stage we are working with NumPy arrays, later on, we will need to convert this NumPy array into a PyTorch tensor with shape [N, 128, 128].\n",
        "\n",
        "Include the result with your PDF submission.\n"
      ],
      "metadata": {
        "id": "Qyxs7WDI3zHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "\n",
        "\n",
        "\n",
        "# Run this code, include the result with your PDF submission!!\n",
        "print(train_data.shape) # if this is [N, 128, 128]\n",
        "print(generate_plots(train_data).shape) # should be [N, 128, 128]\n",
        "plt.imshow(generate_plots(train_data[idx*30])) # should show spectrogram, 30 is just an example.\n",
        "# Please take the first 2 digits of your ID (if both of your ID starts with 0 change it to 1)\n"
      ],
      "metadata": {
        "id": "zLU0GBJ-d_gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (c) -- 3%\n",
        "\n",
        "Why is it important that our data set will be ***balanced***? In other words, suppose we created\n",
        "a data set where 99% of the images are of Gaussian spectrogram, and\n",
        "1% of the images are the other classes. Why could this be a problem?"
      ],
      "metadata": {
        "id": "nRxtj6FV6q1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your explanation here:**\n",
        "\n",
        "\\\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "Lc9HtH77fGz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (d) -- 3%\n",
        "\n",
        "Our neural network will take as input spectrogram images and predict their class. Since we have four string classes we would want to convert them into numbers, where each number is assigned to each class.\n",
        "\n",
        "**Complete** the helper function `convert_class_to_number` so that the function output will be a dictionary that assigns a number to each class.\n",
        "Examples of how this function should operate are detailed in the code below.\n",
        "\n",
        "You can use the defined `vocab`, `lables2num_vocab`,\n",
        "and `num2labels_vocab` in your code."
      ],
      "metadata": {
        "id": "vCELKSFLJ4ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A list of all the labels in the data set. We will assign a unique\n",
        "# identifier for each of these labels.\n",
        "vocab = sorted(list(set([s for s in train_labels]))) # A mapping of index => label (string)\n",
        "num2labels_vocab = dict(enumerate(vocab)) # A mapping of labels => its index\n",
        "labels2num_vocab = {word:index for index, word in num2labels_vocab.items()}\n",
        "\n",
        "def convert_class_to_number(labels):\n",
        "    \"\"\"\n",
        "    This function takes a list of labels\n",
        "    and returns a new list with the same structure, but where each label\n",
        "    is replaced by its index in `num2labels_vocab`.\n",
        "\n",
        "    Example:\n",
        "    >>> convert_class_to_number([['Pulse', 'SingleFrequency', 'Pulse', 'Gaussian', 'ThreeFrequency'], ['ThreeFrequency', 'Pulse', 'Gaussian', 'SingleFrequency'])\n",
        "    [[1, 2, 1, 0, 3], [3, 1, 0, 2]]\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "y8kw-sQcfls6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (e) -- 3%\n",
        "Since the labels in the data are comprised of $4$ distinct classes, our task boils down to classification where the label space $\\mathcal{S}$ is of cardinality $|\\mathcal{S}|=4$ while our input, which is comprised of spectrograms data, is treated as a vector of size $16384\\times 1$.\n",
        "\n",
        "\n",
        "**Implement** yourself a function `create_onehot`, which takes the data in index notation and outputs it in a one-hot notation.\n",
        "\n",
        "Start by reviewing the helper function, which is given to you:"
      ],
      "metadata": {
        "id": "58n8HZwNJu-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_onehot(data):\n",
        "    \"\"\"\n",
        "    Convert one batch of data in the index notation into its corresponding onehot\n",
        "    notation. Remember, the function should work for st.\n",
        "\n",
        "    input - vector with shape D (1D or 2D)\n",
        "    output - vector with shape (D,4)\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "_U-bGPZZfwzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2. Model architecture (30%)\n",
        "\n",
        "In this part we will look at two model architectures: a MultiLayer Perceptron (MLP) and a Convolutional Neural Network (CNN).\n",
        "\n",
        "Since the labels are comprised of $4$ distinct classes, our task boils down to classification where the label space $\\mathcal{S}$ is of cardinality $|\\mathcal{S}|=4$ while our input is treated as a vector of size $128 \\times 128$ (i.e., the spectrogram matrix).\n",
        "\n",
        "We build the model in PyTorch. Since PyTorch uses automatic\n",
        "differentiation, we only need to write the *forward pass* of our\n",
        "model."
      ],
      "metadata": {
        "id": "dXPQhpiNHBeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part (a) -- Multy layer perceptron (MLP) (15%)\n",
        "\n",
        "Please provide a detailed diagram that best describes this model’s architecture. Specify the number of layers, weights,  etc.\n",
        "\n",
        "\n",
        "\n",
        "This link will help you to understand how to upload an image to the google Colab\n",
        "[https://medium.com/analytics-vidhya/embedding-your-image-in-google-colab-markdown-3998d5ac2684](https://)\n",
        "\n",
        "This is an example of how to change the width and height of the image scheme:\n",
        "\n",
        "`<img src=\"image path\" width=\"400px height=\"200px\"\" />`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x2nPVcr_G6pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PyTorchMLP(nn.Module):\n",
        "    def __init__(self, num_hidden=100):\n",
        "        super(PyTorchMLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(128*128, num_hidden)\n",
        "        self.layer2 = nn.Linear(num_hidden, 4)\n",
        "        self.num_hidden = num_hidden\n",
        "    def forward(self, inp):\n",
        "        inp = inp.reshape([-1, 128*128])\n",
        "        # Note that we will be using the nn.CrossEntropyLoss(), which computes the softmax operation internally, as loss criterion\n",
        "        hidden = self.layer1(inp)\n",
        "        output = self.layer2(hidden)\n",
        "        output = torch.nn.functional.log_softmax(output, dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "N4pV2TIpG3yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please show the model scheme here:**\n",
        "\n",
        "\\\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "6qLmfV_y7e1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part (b) -- Convolutional Neural Network (CNN) (15%)\n",
        "\n",
        "\n",
        "The CNN model is given below. Please provide a detailed diagram that best describes this model's architecture. Specify the number of  layers, kernel size, weights, etc.\n",
        "\n",
        "This link will help you to understand how to upload an image to the google colab\n",
        "[https://medium.com/analytics-vidhya/embedding-your-image-in-google-colab-markdown-3998d5ac2684](https://)\n",
        "\n",
        "This is an example of how to change the width and height of the image scheme:\n",
        "\n",
        "`<img src=\"image path\" width=\"400px height=\"200px\"\" />`"
      ],
      "metadata": {
        "id": "sUmMFYjp7dt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNChannel(nn.Module):\n",
        "    def __init__(self, n=8):\n",
        "        super(CNNChannel, self).__init__()\n",
        "        self.n = n\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n, kernel_size=3, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=n, out_channels=2*n, kernel_size=3, stride=1, padding=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=2*n, out_channels=4*n, kernel_size=3, stride=1, padding=2)\n",
        "        self.conv4 = nn.Conv2d(in_channels=4*n, out_channels=8*n, kernel_size=3, stride=1, padding=2)\n",
        "        self.fc1 = nn.Linear(5184, 100)\n",
        "        self.fc2 = nn.Linear(100, 4)\n",
        "\n",
        "    def forward(self, xs, verbose=False):\n",
        "      x = np.expand_dims(xs, axis=1)\n",
        "      x = torch.Tensor(x)\n",
        "      x = self.conv1(x)\n",
        "      x - nn.functional.relu(x)\n",
        "      x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "      x = self.conv2(x)\n",
        "      x = nn.functional.relu(x)\n",
        "      x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "      x = self.conv3(x)\n",
        "      x = nn.functional.relu(x)\n",
        "      x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "      x = self.conv4(x)\n",
        "      x = nn.functional.relu(x)\n",
        "      x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.fc1(x)\n",
        "      x = nn.functional.relu(x)\n",
        "      x = self.fc2(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "HBlWiibj_guF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please show the model's scheme here:**\n",
        "\n",
        "\\\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "Rwh8CuCGBhwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `estimate_accuracy` is written for you. Depending on how you set up your model and training, you may need to modify this function."
      ],
      "metadata": {
        "id": "fD9Y8xbZ9m1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_accuracy(model, data, label, batch_size=100, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "    # get a batch of data\n",
        "      xt, st = get_batch(data, label, i, i + batch_size, onehot=False)\n",
        "    # forward pass prediction\n",
        "      y = model(torch.Tensor(xt))\n",
        "      y = y.detach().numpy() # convert the PyTorch tensor => numpy array\n",
        "      pred = np.argmax(y, axis=1)\n",
        "      true = np.argmax(st, axis=1)\n",
        "      for ii in range(len(y)):\n",
        "        if pred[ii] == true[ii]:\n",
        "          correct += 1\n",
        "      N += st.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "      if N > max_N:\n",
        "        break\n",
        "    return correct / N"
      ],
      "metadata": {
        "id": "SQscOUGRDbo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function `get_batch` will take as input the whole dataset and output a single batch for the training. The output size of the batch is explained below."
      ],
      "metadata": {
        "id": "HHMFAFsIDX0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, label, range_min, range_max, onehot):\n",
        "    \"\"\"\n",
        "    Convert one batch of data into input and output\n",
        "    data and return the training data (xt, st) where:\n",
        "     - `xt` is an numpy array of one-hot vectors of shape [batch_size, 128, 128]\n",
        "     - `st` is either\n",
        "            - a numpy array of shape [batch_size, 4] if onehot is True,\n",
        "            - a numpy array of shape [batch_size] containing indicies otherwise\n",
        "\n",
        "    Preconditions:\n",
        "     - `data` is a numpy array of shape [N, 128, 128] produced by a call\n",
        "        to `process_data`\n",
        "     - range_max > range_min\n",
        "    \"\"\"\n",
        "    xt = data[range_min:range_max]\n",
        "    st = label[range_min:range_max]\n",
        "    st = convert_class_to_number(st)\n",
        "    if onehot:\n",
        "        st = create_onehot(st).reshape(-1, 4)\n",
        "    return xt, st"
      ],
      "metadata": {
        "id": "_0QEWTk9DZ3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3. Training (34%)\n",
        "\n",
        "Now, we will write the functions required to train the PyTorch models using the Adam optimizer and the cross entropy loss.\n",
        "\n",
        "Our task is a multi-class classification problem. Therefore, we will use a one-hot vector to represent our target.\n"
      ],
      "metadata": {
        "id": "1zrxKK2aE_Nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (a) -- 15%\n",
        "\n",
        "**Complete** the function `train_model`, and use it to train your PyTorch MLP and CNN models.\n",
        "\n",
        "\n",
        "Plot the learning curve using the `plot_learning_curve` function provided\n",
        "to you, and include your plot in your PDF submission.\n",
        "\n",
        "It is also recommended to checkpoint your model (save a copy) after every epoch."
      ],
      "metadata": {
        "id": "yCJh26TXdnXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,\n",
        "                train_data=train_data,\n",
        "                train_label=train_labels,\n",
        "                validation_data=valid_data,\n",
        "                validation_label=valid_labels,\n",
        "                batch_size=100,\n",
        "                learning_rate=0.001,\n",
        "                weight_decay=0,\n",
        "                max_iters=1000,\n",
        "                checkpoint_path=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Train the PyTorch model on the dataset `train_data`, reporting\n",
        "    the validation accuracy on `validation_data`, for `max_iters`\n",
        "    iteration.\n",
        "\n",
        "    If you want to **checkpoint** your model weights (i.e. save the\n",
        "    model weights to Google Drive), then the parameter\n",
        "    `checkpoint_path` should be a string path with `{}` to be replaced\n",
        "    by the iteration count:\n",
        "\n",
        "    For example, calling\n",
        "\n",
        "    >>> train_model(model, ...,\n",
        "            checkpoint_path = '/content/gdrive/My Drive/assignment2/mlp/ckpt-{}.pk')\n",
        "\n",
        "    will save the model parameters in Google Drive every 100 iterations.\n",
        "    You will have to make sure that the path exists (i.e. you'll need to create\n",
        "    the folder Intro_to_Deep_Learning, mlp or cnn, etc...). Your Google Drive will be populated with files:\n",
        "\n",
        "    - /content/gdrive/My Drive/assignment2/mlp/ckpt-500.pk\n",
        "    - /content/gdrive/My Drive/assignment2/cnn/ckpt-1000.pk\n",
        "    - ...\n",
        "\n",
        "    To load the weights at a later time, you can run:\n",
        "\n",
        "    >>> model.load_state_dict(torch.load('/content/gdrive/My Drive/assignment2/mlp/ckpt-500.pk'))\n",
        "\n",
        "    This function returns the training loss, and the training/validation accuracy,\n",
        "    which we can use to plot the learning curve.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay)\n",
        "\n",
        "    iters, losses = [], []\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "\n",
        "    n = 0 # the number of iterations\n",
        "    while True:\n",
        "      reindex = np.random.permutation(len(train_data))\n",
        "      train_data = train_data[reindex]\n",
        "      train_labels = train_label[reindex]\n",
        "      for i in range(0, train_data.shape[0], batch_size):\n",
        "          if (i + batch_size) > train_data.shape[0]:\n",
        "              break\n",
        "          model.train()\n",
        "          # get the input and targets of a minibatch\n",
        "          xt, st = get_batch(train_data, train_label, i, i + batch_size, onehot=False)\n",
        "          # convert from numpy arrays to PyTorch tensors\n",
        "          xt = torch.Tensor(xt)\n",
        "          st = torch.Tensor(st)\n",
        "\n",
        "          zs = ...                 # compute prediction logit\n",
        "          loss =                   # compute the total loss\n",
        "          ...                      # compute updates for each parameter\n",
        "          ...                      # make the updates for each parameter\n",
        "          ...                      # a clean up step for PyTorch\n",
        "\n",
        "          # save the current training information\n",
        "          iters.append(n)\n",
        "          losses.append(float(loss)/batch_size)  # compute *average* loss\n",
        "          if n % 100 == 0:\n",
        "              iters_sub.append(n)\n",
        "              train_cost = float(loss.detach().numpy())\n",
        "              train_acc = estimate_accuracy(model, train_data, train_label)\n",
        "              train_accs.append(train_acc)\n",
        "              val_acc = estimate_accuracy(model, validation_data, validation_label)\n",
        "              val_accs.append(val_acc)\n",
        "              print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "                    n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "              if (checkpoint_path is not None) and n > 0:\n",
        "                  torch.save(model.state_dict(), checkpoint_path.format(n))\n",
        "\n",
        "          # increment the iteration number\n",
        "          n += 1\n",
        "\n",
        "          if n > max_iters:\n",
        "              return iters, losses, iters_sub, train_accs, val_accs\n",
        "\n",
        "\n",
        "def plot_learning_curve(iters, losses, iters_sub, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "    \"\"\"\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "    plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U3joOVaXINXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (b) -- 15%\n",
        "\n",
        "Train your models from Questions 2(a) and 2(b). Change the values of a few\n",
        "hyperparameters, including the learning rate, batch size, choice of $n$ and the kernel size in the CNN model, choice of $num$_$hidden$ in the MLP model. You do not need to check all values for all hyperparameters. Instead, try to make significant changes to see how each change affects your scores\n",
        "(try to start with finding a reasonable learning rate for each network, then start changing the other parameters).\n",
        "\n",
        "In this section, explain how you tuned your hyperparameters."
      ],
      "metadata": {
        "id": "kzH235f8NYBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your explanation here:**\n",
        "\n",
        "\\\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "Af1OS3uvOXGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Include the training curves for the two models:**"
      ],
      "metadata": {
        "id": "DS8qvV2OOeR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_mlp = PyTorchMLP()\n",
        "# learning_curve_info = train_model(pytorch_mlp, ...)\n",
        "\n",
        "\n",
        "# plot_learning_curve(*learning_curve_info)"
      ],
      "metadata": {
        "id": "CK-FEbXKRCXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn_ch = CNNChannel()\n",
        "# learning_curve_info = train_model(pytorch_mlp, ...)\n",
        "\n",
        "\n",
        "# plot_learning_curve(*learning_curve_info)"
      ],
      "metadata": {
        "id": "z61kXyKBt82A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (c) -- 4%\n",
        "\n",
        "Include your training curves for the **best** models from each MLP and CNN.\n",
        "These are the models that you will use in Question 4."
      ],
      "metadata": {
        "id": "jaLtjfAcW7L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_mlp = PyTorchMLP()\n",
        "print('The model we used here is MLP channel model')\n",
        "learning_curve_info = run_pytorch_gradient_descent(pytorch_mlp,\n",
        "                                 train_data=train_data,\n",
        "                                 train_label=train_ts_onehot,\n",
        "                                 validation_data=valid_data,\n",
        "                                 validation_label=valid_ts_onehot,\n",
        "                                 batch_size=100,\n",
        "                                 learning_rate=0.001,\n",
        "                                 weight_decay=0,\n",
        "                                 max_iters=1000,\n",
        "                                 checkpoint_path=None)\n",
        "\n",
        "\n",
        "plot_learning_curve(*learning_curve_info)\n",
        "learning_curve_info = train_model(model_cnn_ch, train_data, valid_data, batch_size=64, learning_rate=0.001, weight_decay=0, max_iters=120, checkpoint_path=None)\n",
        "plot_learning_curve(*learning_curve_info)"
      ],
      "metadata": {
        "id": "rI5hBOK6XWBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Include the training curves for the two models.\n",
        "model_cnn_ch = CNNChannel()\n",
        "print('The model we used here is CNN model')\n",
        "learning_curve_info = train_model(model_cnn_ch, train_data, train_ts_onehot, valid_data, valid_ts_onehot,\n",
        "                                  batch_size=25, learning_rate=0.001, weight_decay=0, max_iters=50, checkpoint_path=None)\n",
        "plot_learning_curve(*learning_curve_info)"
      ],
      "metadata": {
        "id": "JAZJm0EtXPg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4. Testing (21%)\n"
      ],
      "metadata": {
        "id": "XXcLo8bTiaKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Report the test accuracies of your **single best** model,\n",
        "separately for the test set.\n",
        "Do this by choosing the model\n",
        "architecture that produces the best validation accuracy. For instance,\n",
        "if your model attained the\n",
        "best validation accuracy in epoch 10, then the weights at epoch 10 is what you should be using\n",
        "to report the test accuracy."
      ],
      "metadata": {
        "id": "0hDl58XJX68j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure to include the test accuracy in your report!!\n",
        "# Write your code here:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9o63BEYbYdnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "For each model, display one of the signal spectrograms that your model correctly classified, and one of the signal spectrograms that your model classified incorrectly."
      ],
      "metadata": {
        "id": "t2AFE0zwYcey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure to include the test accuracy in your report!!\n",
        "# Write your code here:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LTyoQO9yY-sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Compare the capacity, the number of layers, and performance of the two architectures, and discuss the advantages and disadvantages between these architectures.\n",
        "\n",
        "Will one of these models perform better? Explain why.\n",
        "\n",
        "Is the architecture choices important in machine learning?"
      ],
      "metadata": {
        "id": "CEOT7sb78_cE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your explanation here:**\n",
        "\n",
        "\\\n",
        "\n",
        "\\\n",
        "\n"
      ],
      "metadata": {
        "id": "wLgttUY79Cqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF export\n",
        "To export a PDF of the completed notebook, you might find the following helper functions helpful. Here are some resources for additional learning.\n",
        "\n",
        "- https://nbconvert.readthedocs.io/en/latest/\n",
        "\n",
        "- https://nbconvert.readthedocs.io/en/latest/install.html#installing-tex"
      ],
      "metadata": {
        "id": "_F1TojWCiuPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic"
      ],
      "metadata": {
        "id": "aYwMZxvEjTR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to pdf /content/drive/MyDrive/Colab_Notebooks/Assignment2/Assignment2.ipynb\n",
        "# TODO - UPDATE ME WITH THE TRUE PATH! and UPDATE THE FILE NAME."
      ],
      "metadata": {
        "id": "Rp68ryzEjXDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}